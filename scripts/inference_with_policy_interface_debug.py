#!/usr/bin/env python3
"""
åŸºäºç›¸æœºå’ŒACTæ¨¡å‹çš„å®æ—¶æ¨ç†è„šæœ¬ - è°ƒè¯•ç‰ˆæœ¬
é€‚é…æœ€æ–°ç‰ˆæœ¬çš„lerobotåº“ï¼Œé›†æˆæ¨ç†æ•°æ®ä¿å­˜åŠŸèƒ½
"""

import os
from shlex import join
import numpy as np
import torch
import time
from pathlib import Path
import argparse
from PIL import Image
import cv2
import math
from safetensors.torch import load_file
import sys
import yaml
from common.gripper_util import convert_gripper_width_to_encoder

# å¯¼å…¥debug_logger
from debug_logger import InferenceLogger, AnomalyDetector


# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.pathï¼Œç¡®ä¿ä¼˜å…ˆä½¿ç”¨é¡¹ç›®ä¸­çš„lerobotåº“
project_dir = Path(__file__).parent.parent
model_lerobot_path = project_dir / "model" / "lerobot" / "src"
sys.path.insert(0, str(model_lerobot_path))
sys.path.insert(0, str(project_dir))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

# å¯¼å…¥æœ€æ–°ç‰ˆæœ¬çš„lerobotåº“
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.constants import OBS_IMAGES, ACTION, OBS_STATE

# å¯¼å…¥PolicyInterface
from policy_interface import create_policy_interface

# å¯¼å…¥precise_wait
from common.precise_sleep import precise_wait

# ç›¸æœºç›¸å…³å¯¼å…¥
import pyrealsense2 as rs
from r3kit.devices.camera.realsense import config as rs_cfg
from r3kit.devices.camera.realsense.d415 import D415
R3KIT_RS_AVAILABLE = True

# D415 ç›¸æœºé…ç½®ï¼ˆä¸é‡‡é›†è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
FPS = 30
D415_CAMERAS = {   
    "cam4": "327322062498",  # å›ºå®šæœºä½è§†è§’
    "eih": "038522062288",   # eye-in-handè§†è§’ï¼ˆéœ€è¦æ ¹æ®å®é™…åºåˆ—å·ä¿®æ”¹ï¼‰
}

class CameraSystem:
    """ç›¸æœºç³»ç»Ÿæ¥å£ - ä»inference_poly1å¤ç”¨"""
    
    def __init__(self):
        self.cameras = {}
        self.camera_names = ["cam4", "eih"]  # æ”¯æŒåŒè§†è§’
        self.use_realsense = True
        
        # ä¸é‡‡é›†è„šæœ¬ä¿æŒä¸€è‡´çš„æµé…ç½®
        rs_cfg.D415_STREAMS = [
            (rs.stream.depth, 640,480, rs.format.z16, FPS),
            (rs.stream.color, 640,480, rs.format.bgr8, FPS),
        ]
        for name in self.camera_names:
            serial = D415_CAMERAS.get(name)
            if serial is None:
                print(f"{name} ç¼ºå°‘åºåˆ—å·ï¼Œè·³è¿‡")
                continue
            try:
                cam = D415(id=serial, depth=True, name=name)
                self.cameras[name] = cam
                print(f"æˆåŠŸåˆå§‹åŒ–ç›¸æœº {name} (åºåˆ—å·: {serial})")
            except Exception as e:
                print(f"åˆå§‹åŒ–ç›¸æœº {name} å¤±è´¥: {e}")
                continue
                
        if len(self.cameras) > 0:
            self.use_realsense = True
            print(f"ä½¿ç”¨ RealSense D415ï¼Œç›¸æœºæ•°é‡: {len(self.cameras)}")
            print(f"å¯ç”¨ç›¸æœº: {list(self.cameras.keys())}")
        else:
            print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸåˆå§‹åŒ–ä»»ä½•ç›¸æœº")
    
    def get_image(self, cam_name):
        """è·å–æŒ‡å®šç›¸æœºçš„å›¾åƒ"""
        if cam_name not in self.cameras:
            return None
        
        try:
            if self.use_realsense:
                # r3kit D415 æ¥å£
                color, depth = self.cameras[cam_name].get()
                if color is None:
                    return None
                # è½¬ RGBï¼ˆä¸‹æ¸¸é¢„å¤„ç†é»˜è®¤ä»¥ RGB å¤„ç†ï¼‰
                frame_rgb = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)
                return frame_rgb
            else:
                # OpenCV æ‘„åƒå¤´
                ret, frame = self.cameras[cam_name].read()
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    return frame_rgb
                return None
        except Exception as e:
            print(f"è·å– {cam_name} å›¾åƒå¤±è´¥: {e}")
            return None
    
    def get_all_images(self):
        """è·å–æ‰€æœ‰ç›¸æœºçš„å›¾åƒ"""
        images = {}
        for cam_name in self.camera_names:
            image = self.get_image(cam_name)
            if image is not None:
                images[cam_name] = image
            else:
                # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒä½œä¸º fallback
                images[cam_name] = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                print(f"è­¦å‘Š: {cam_name} ç›¸æœºå›¾åƒè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾åƒ")
        
        return images
    
    def close(self):
        """å…³é—­æ‰€æœ‰ç›¸æœº"""
        for cam_name, cap in self.cameras.items():
            try:
                if self.use_realsense:
                    # D415 ç±»å¯èƒ½æ²¡æœ‰ stop æ–¹æ³•ï¼Œä½¿ç”¨ __del__ æˆ–è€…ä¸åšä»»ä½•æ“ä½œ
                    if hasattr(cap, 'stop'):
                        cap.stop()
                    elif hasattr(cap, 'close'):
                        cap.close()
                    # å¯¹äº r3kit D415ï¼Œé€šå¸¸ç”±ææ„å‡½æ•°è‡ªåŠ¨å¤„ç†
                else:
                    cap.release()
                print(f"{cam_name} å·²å…³é—­")
            except Exception as e:
                print(f"å…³é—­ {cam_name} å¤±è´¥: {e}")


class ACTPolicyWrapper:
    """ACTç­–ç•¥åŒ…è£…å™¨ - é€‚é…æœ€æ–°ç‰ˆæœ¬çš„lerobotåº“"""
    
    def __init__(self, model_path, device="cpu", camera_system=None, debug_image=False):
        self.device = torch.device(device)
        self.model_path = Path(model_path)
        self.camera_system = camera_system
        self.debug_image = debug_image
        
        # é…ç½®å‚æ•°
        self.image_size = (224, 224)
        self.camera_names = ["cam4", "eih"]  # æ”¯æŒåŒè§†è§’
        self.joint_dim = 7  # 7ä¸ªå…³èŠ‚è§’åº¦ï¼ˆå¼§åº¦ï¼‰  
        self.gripper_dim = 1  # 1ä¸ªå¤¹çˆªå¼€åˆå€¼  
        self.action_dim = self.joint_dim + self.gripper_dim  # æ€»å…±8ç»´  
        self.chunk_size = 32  # ACTæ¨¡å‹çš„chunkå¤§å°
        
        # åŠ è½½æ¨¡å‹
        self.policy = self._load_policy()
        
        print(f"ACTç­–ç•¥åˆå§‹åŒ–å®Œæˆ: {model_path}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ”¯æŒåŒè§†è§’è¾“å…¥: å›ºå®šæœºä½(cam4) + eye-in-hand(eih)")
        print(f"ç›¸æœºç³»ç»ŸçŠ¶æ€: {len(self.camera_system.cameras) if self.camera_system else 0} ä¸ªç›¸æœºå·²åˆå§‹åŒ–")
    
    def _load_policy(self):
        """åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹"""
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        # ä½¿ç”¨from_pretrainedåŠ è½½æ¨¡å‹(æ¨èæ–¹å¼)
        policy = ACTPolicy.from_pretrained(
            pretrained_name_or_path=str(self.model_path)
        )
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        policy.to(self.device)
        
        # è®¾ç½®æ‰§è¡Œéƒ¨ç½²
        policy.config.n_action_steps = 50

        # æ‰“å°é…ç½®ä¿¡æ¯
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸ:")
        print(f" ç­–ç•¥ç±»å‹: {policy.config.type}")
        print(f" è®¾å¤‡: {next(policy.parameters()).device}")
        print(f" æ—¶é—´é›†æˆç³»æ•°: {policy.config.temporal_ensemble_coeff}")
        print(f" åŠ¨ä½œæ­¥æ•°: {policy.config.n_action_steps}")
        print(f" å—å¤§å°: {policy.config.chunk_size}")
        
        return policy
    
    def preprocess_image(self, image, debug=False):
        """é¢„å¤„ç†å›¾åƒ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼šå…ˆè£å‰ªæˆæ­£æ–¹å½¢ï¼Œå†ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸"""
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        # è·å–åŸå§‹å›¾åƒå°ºå¯¸
        width, height = image.size
        if debug:
            print(f"åŸå§‹å›¾åƒå°ºå¯¸: {width}x{height}")
        
        # å…ˆè£å‰ªæˆæ­£æ–¹å½¢ï¼ˆå–è¾ƒå°çš„è¾¹ä½œä¸ºè¾¹é•¿ï¼‰
        if width > height:
            # å®½åº¦å¤§äºé«˜åº¦ï¼Œä»ä¸­å¿ƒè£å‰ª
            left = (width - height) // 2
            right = left + height
            top = 0
            bottom = height
        else:
            # é«˜åº¦å¤§äºç­‰äºå®½åº¦ï¼Œä»ä¸­å¿ƒè£å‰ª
            top = (height - width) // 2
            bottom = top + width
            left = 0
            right = width
        
        # è£å‰ªæˆæ­£æ–¹å½¢
        image_cropped = image.crop((left, top, right, bottom))
        if debug:
            print(f"è£å‰ªåå°ºå¯¸: {image_cropped.size}")
        
        # ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
        image_resized = image_cropped.resize(self.image_size, Image.Resampling.LANCZOS)
        if debug:
            print(f"ç¼©æ”¾åå°ºå¯¸: {image_resized.size}")
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
        image_tensor = torch.from_numpy(np.array(image_resized)).permute(2, 0, 1).float()  # (3, H, W)
        image_tensor = image_tensor / 255.0
        
        return image_tensor
    
    def get_current_state_with_gripper(self, obs):
        """ä»è§‚æµ‹ä¸­è·å–å½“å‰çŠ¶æ€ï¼ˆ8ç»´ï¼‰"""
        # ä»è§‚æµ‹ä¸­æå–å…³èŠ‚ä½ç½®ï¼ˆå¼§åº¦ï¼‰
        joints_rad = obs['robot0_joint_pos']  # (7,)
        
        # è·å–å¤¹çˆªå®½åº¦ï¼ˆä»è§‚æµ‹ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        if 'robot0_gripper_width' in obs:
            gripper_width = obs['robot0_gripper_width']
            if isinstance(gripper_width, np.ndarray):
                gripper_width = gripper_width[0] if len(gripper_width) > 0 else 0.04
        else:
            gripper_width = 0.04  # é»˜è®¤å¤¹çˆªå®½åº¦ï¼ˆç±³ï¼‰
        
        # è¿”å›8ç»´çŠ¶æ€ï¼š7ä¸ªå…³èŠ‚è§’åº¦ï¼ˆå¼§åº¦ï¼‰ + 1ä¸ªå¤¹çˆªå®½åº¦ï¼ˆç±³ï¼‰
        return np.concatenate([joints_rad, [gripper_width]])
    
    def predict_single_action(self, images, current_state):
        """
        å•æ­¥é¢„æµ‹åŠ¨ä½œï¼ˆä½¿ç”¨ ACTPolicy.select_actionï¼‰ã€‚
        è¿”å›: (8,) numpy æ•°ç»„ï¼Œå‰7ç»´ä¸ºå…³èŠ‚(å¼§åº¦)ï¼Œç¬¬8ç»´ä¸ºå¤¹çˆª(ç±³)ã€‚
        """
        # é¢„å¤„ç†å›ºå®šæœºä½è§†è§’å›¾åƒ
        if "cam4" in images:
            color_img_tensor = self.preprocess_image(images["cam4"], debug=self.debug_image)
        else:
            # éšæœºå›¾åƒå›é€€
            fake = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            color_img_tensor = self.preprocess_image(fake, debug=self.debug_image)
            print("è­¦å‘Š: å›ºå®šæœºä½è§†è§’å›¾åƒè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾åƒ")
        
        # é¢„å¤„ç†eye-in-handè§†è§’å›¾åƒ
        if "eih" in images:
            eih_img_tensor = self.preprocess_image(images["eih"], debug=self.debug_image)
        else:
            # éšæœºå›¾åƒå›é€€
            fake = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            eih_img_tensor = self.preprocess_image(fake, debug=self.debug_image)
            print("è­¦å‘Š: eye-in-handè§†è§’å›¾åƒè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾åƒ")
        
        # æ„å»ºbatch - ä½¿ç”¨æ–°ç‰ˆæœ¬çš„æ ¼å¼
        batch = {
            "observation.image.color": color_img_tensor.unsqueeze(0).to(self.device),
            "observation.image.eih": eih_img_tensor.unsqueeze(0).to(self.device),
            "observation.state": torch.tensor(current_state, dtype=torch.float32).unsqueeze(0).to(self.device),
        }
        
        with torch.no_grad():
            # ä½¿ç”¨select_actionè¿›è¡Œå•æ­¥é¢„æµ‹
            action = self.policy.select_action(batch)  # (1, action_dim)ï¼Œå·²åå½’ä¸€åŒ–
            action = action.squeeze(0).detach().cpu().numpy()  # (8,)
        
        # ç›´æ¥è¿”å›æ¨¡å‹è¾“å‡ºï¼Œæ‰€æœ‰å•ä½éƒ½æ˜¯å¼§åº¦ï¼ˆå‰7ç»´ï¼‰å’Œç±³ï¼ˆç¬¬8ç»´ï¼‰
        return action
    
    
    def __call__(self, obs):
        """
        ç­–ç•¥å‡½æ•° - PolicyInterfaceå…¼å®¹æ¥å£
        
        Args:
            obs: è§‚æµ‹å­—å…¸ï¼ŒåŒ…å«robot0_joint_posç­‰
            
        Returns:
            action: 7ç»´å…³èŠ‚åŠ¨ä½œ [j1, j2, j3, j4, j5, j6, j7] (å¼§åº¦)
        """
        # è·å–å½“å‰å›¾åƒ
        current_images = self.camera_system.get_all_images()
        
        # è·å–å½“å‰çŠ¶æ€
        current_state = self.get_current_state_with_gripper(obs)
        
        # å•æ­¥é¢„æµ‹åŠ¨ä½œ
        full_action = self.predict_single_action(current_images, current_state)
        
        joint_action = full_action[:self.joint_dim]
        # è·å–gripperåŠ¨ä½œï¼ˆç¬¬8ç»´ï¼‰
        gripper_width = full_action[self.joint_dim] - 0.005 # å¤¹çˆªå®½åº¦ï¼ˆç±³ï¼‰

        gripper_encoder = convert_gripper_width_to_encoder(gripper_width)

        cur_action = np.concatenate([joint_action, [gripper_encoder]])
        return cur_action
    
    def check_camera_status(self):
        """æ£€æŸ¥ç›¸æœºçŠ¶æ€"""
        if not self.camera_system:
            print("ç›¸æœºç³»ç»Ÿæœªåˆå§‹åŒ–")
            return False
        
        print("ç›¸æœºçŠ¶æ€æ£€æŸ¥:")
        for cam_name in self.camera_names:
            if cam_name in self.camera_system.cameras:
                print(f"  âœ… {cam_name}: å·²åˆå§‹åŒ–")
            else:
                print(f"  âŒ {cam_name}: æœªåˆå§‹åŒ–")
        
        return len(self.camera_system.cameras) > 0


class ACTInferenceRunner:
    """ACTæ¨ç†è¿è¡Œå™¨ - ä½¿ç”¨ä¸replay_trajectoryç›¸åŒçš„æ¥å£å½¢å¼"""
    
    def __init__(self, 
                 model_path: str,
                 config_path: str,
                 device: str = "cuda",
                 max_steps: int = 1000,
                 test_mode: bool = False,
                 frequency: float = 20.0,
                 debug_image: bool = False,
                 debug_log_dir: str = "debug_logs",
                 save_frequency: int = 1):
        """
        åˆå§‹åŒ–ACTæ¨ç†è¿è¡Œå™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            max_steps: æœ€å¤§è¿è¡Œæ­¥æ•°
            test_mode: æµ‹è¯•æ¨¡å¼
            frequency: æ¨ç†é¢‘ç‡ (Hz)
            debug_image: æ˜¯å¦æ˜¾ç¤ºå›¾åƒå¤„ç†è°ƒè¯•ä¿¡æ¯
            debug_log_dir: è°ƒè¯•æ—¥å¿—ä¿å­˜ç›®å½•
            save_frequency: æ•°æ®ä¿å­˜é¢‘ç‡ï¼ˆæ¯Næ­¥ä¿å­˜ä¸€æ¬¡ï¼‰
        """
        self.model_path = model_path
        self.config_path = config_path
        self.device = device
        self.max_steps = max_steps
        self.test_mode = test_mode
        self.frequency = frequency
        self.debug_image = debug_image
        self.dt = 1.0 / frequency  # æ—¶é—´é—´éš”
        
        # åˆ›å»ºç›¸æœºç³»ç»Ÿ
        self.camera_system = CameraSystem()
        
        # åˆ›å»ºACTç­–ç•¥
        self.policy = ACTPolicyWrapper(
            model_path=model_path,
            device=device,
            camera_system=self.camera_system,
            debug_image=self.debug_image
        )
        
        # åˆå§‹åŒ–è°ƒè¯•è®°å½•å™¨
        self.logger = InferenceLogger(
            log_dir=debug_log_dir,
            save_frequency=save_frequency,
            save_images=True,
            max_logs=10000
        )
        
        # åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
        self.detector = AnomalyDetector(
            action_threshold=0.5,
            inference_time_threshold=0.1,
            gripper_threshold=10
        )
        
        print(f"ACTæ¨ç†è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"é…ç½®æ–‡ä»¶: {config_path}")
        print(f"è®¾å¤‡: {device}")
        print(f"æµ‹è¯•æ¨¡å¼: {test_mode}")
        print(f"æ¨ç†é¢‘ç‡: {frequency} Hz")
        print(f"è°ƒè¯•æ—¥å¿—ç›®å½•: {debug_log_dir}")
        print(f"ä¿å­˜é¢‘ç‡: æ¯{save_frequency}æ­¥")
        
        # æ£€æŸ¥ç›¸æœºçŠ¶æ€
        self.policy.check_camera_status()
    
    def run(self):
        """æ‰§è¡Œæ¨ç†"""
        if self.test_mode:
            print("ä½¿ç”¨æµ‹è¯•æ¨¡å¼")
            self._run_test_mode()
        else:
            print("ä½¿ç”¨å®æ—¶æ¨ç†æ¨¡å¼")
            self._run_real_time_mode()
    
    def _run_test_mode(self):
        """æµ‹è¯•æ¨¡å¼ï¼šè¿è¡Œå‡ æ¬¡æ¨ç†"""
        print("å¼€å§‹æµ‹è¯•æ¨ç†...")
        for i in range(3):
            print(f"\n=== æµ‹è¯•æ¨ç† {i + 1} ===")
            # æ¨¡æ‹Ÿè§‚æµ‹æ•°æ®
            obs = {
                'robot0_joint_pos': np.random.uniform(-1, 1, 7),
                'robot0_joint_vel': np.random.uniform(-0.1, 0.1, 7),
                'robot0_eef_pos': np.random.uniform(0.3, 0.7, 3),
                'robot0_eef_rot_axis_angle': np.random.uniform(-1, 1, 3),
                'robot0_gripper_width': np.random.uniform(0.0, 0.08, 1),  # æ·»åŠ gripperå®½åº¦
                'timestamp': time.monotonic()
            }
            
            # è·å–å›¾åƒæ•°æ®
            current_images = self.camera_system.get_all_images()
            
            # æ‰§è¡Œç­–ç•¥
            t_start = time.monotonic()
            cur_action = self.policy(obs)
            t_end = time.monotonic()
            
            joint_action = cur_action[:self.policy.joint_dim]
            gripper_action = cur_action[self.policy.joint_dim]
            
            print(f"é¢„æµ‹çš„å…³èŠ‚åŠ¨ä½œï¼ˆ7ç»´ï¼‰: {joint_action}")
            print(f"é¢„æµ‹çš„å¤¹çˆªåŠ¨ä½œï¼ˆ1ç»´ï¼‰: {gripper_action}")
            print(f"é¢„æµ‹çš„å®Œæ•´åŠ¨ä½œï¼ˆ8ç»´ï¼‰: {cur_action}")
            
            # è®°å½•è°ƒè¯•æ•°æ®
            input_data = {
                "cam_image": current_images.get("cam4"),
                "eih_image": current_images.get("eih"),
                "robot_state": obs['robot0_joint_pos'],
                "gripper_state": obs['robot0_gripper_width'][0]
            }
            
            output_data = {
                "joint_action": joint_action,
                "gripper_action": gripper_action,
                "gripper_width": obs['robot0_gripper_width'][0],
                "full_action": cur_action
            }
            
            metadata = {
                "inference_time": t_end - t_start,
                "step": i,
                "test_mode": True
            }
            
            # ä¿å­˜è®°å½•
            record_id = self.logger.log_inference(input_data, output_data, metadata)
            print(f"è°ƒè¯•è®°å½•å·²ä¿å­˜: {record_id}")
            
            # å¼‚å¸¸æ£€æµ‹
            anomalies = self.detector.detect_anomalies(input_data, output_data, metadata)
            if anomalies:
                print(f"âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸: {anomalies}")
            
            time.sleep(2)
    
    def _run_real_time_mode(self):
        """å®æ—¶æ¨ç†æ¨¡å¼"""
        try:
            # åˆ›å»ºç­–ç•¥æ¥å£
            interface = create_policy_interface(self.config_path, self.policy)
            
            print("å¯åŠ¨ç­–ç•¥æ¥å£...")
            interface.start()
            print("ç­–ç•¥æ¥å£å·²å¯åŠ¨!")
            
            # è·å–åˆå§‹è§‚æµ‹
            obs = interface.get_observation()
            print(f"åˆå§‹å…³èŠ‚ä½ç½®: {obs['robot0_joint_pos']}")
            print(f"åˆå§‹Gripperå®½åº¦: {obs['robot0_gripper_width']}")
            
            # è¿è¡Œç­–ç•¥
            print(f"\nå¼€å§‹è¿è¡Œç­–ç•¥...")
            print(f"æ¨ç†é¢‘ç‡: {self.frequency} Hz (dt = {self.dt:.3f}s)")
            print("æŒ‰ Ctrl+C åœæ­¢")
            
            # åˆå§‹åŒ–æ—¶é—´æ§åˆ¶
            t_start = time.monotonic()
            step = 0
            
            # è¶…æ—¶é™çº§ç­–ç•¥ç›¸å…³å˜é‡
            last_joint_action = None
            last_gripper_action = None
            inference_times = []
            max_inference_time = 0.18  # æœ€å¤§å…è®¸æ¨ç†æ—¶é—´ (180ms) - é’ˆå¯¹130msæ¨ç†æ—¶é—´ä¼˜åŒ–
            timeout_count = 0
            
            while True:
                if self.max_steps is not None and step >= self.max_steps:
                    print(f"è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.max_steps}ï¼Œåœæ­¢è¿è¡Œ")
                    break
                
                # è®¡ç®—å½“å‰å‘¨æœŸç»“æŸæ—¶é—´
                t_cycle_end = t_start + (step + 1) * self.dt
                t_cycle_start = time.monotonic()
                
                # è·å–è§‚æµ‹
                obs = interface.get_observation()
                
                # è·å–å½“å‰å›¾åƒ
                current_images = self.camera_system.get_all_images()
                
                # æ‰§è¡Œç­–ç•¥ - æ·»åŠ è¶…æ—¶æ£€æŸ¥
                t_inference_start = time.monotonic()
                try:
                    cur_action = self.policy(obs)
                    joint_action = cur_action[:self.policy.joint_dim]
                    gripper_action = cur_action[self.policy.joint_dim]
                    t_inference_end = time.monotonic()
                    inference_time = t_inference_end - t_inference_start
                    inference_times.append(inference_time)
                    
                    # æ›´æ–°æœ€åæœ‰æ•ˆçš„åŠ¨ä½œ
                    last_joint_action = cur_action.copy()
                    last_gripper_action = cur_action[self.policy.joint_dim]
                    timeout_count = 0
                    
                except Exception as e:
                    print(f"æ¨ç†å¤±è´¥: {e}")
                    t_inference_end = time.monotonic()
                    inference_time = t_inference_end - t_inference_start
                    inference_times.append(inference_time)
                    timeout_count += 1
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.monotonic()
                elapsed_time = current_time - t_cycle_start
                remaining_time = t_cycle_end - current_time
                
                # å¦‚æœæ¨ç†æ—¶é—´è¿‡é•¿æˆ–å‰©ä½™æ—¶é—´ä¸è¶³ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
                if (inference_time > max_inference_time or 
                    remaining_time < 0.01 or  # å‰©ä½™æ—¶é—´å°‘äº10ms
                    timeout_count > 0):
                    
                    if last_joint_action is not None and last_gripper_action is not None:
                        # ä½¿ç”¨ä¸Šæ¬¡çš„æœ‰æ•ˆåŠ¨ä½œ
                        joint_action = last_joint_action
                        gripper_action = last_gripper_action
                        print(f"âš ï¸  ä½¿ç”¨é™çº§ç­–ç•¥: æ¨ç†æ—¶é—´={inference_time:.3f}s, å‰©ä½™æ—¶é—´={remaining_time:.3f}s")
                    else:
                        # å¦‚æœæ²¡æœ‰ä»»ä½•æœ‰æ•ˆåŠ¨ä½œï¼Œä½¿ç”¨å½“å‰ä½ç½®
                        joint_action = obs['robot0_joint_pos']
                        gripper_action = 128  # é»˜è®¤gripperä½ç½®
                        print(f"âš ï¸  ä½¿ç”¨å½“å‰ä½ç½®: æ¨ç†æ—¶é—´={inference_time:.3f}s")
                
                # è®°å½•è°ƒè¯•æ•°æ®
                input_data = {
                    "cam_image": current_images.get("cam4"),
                    "eih_image": current_images.get("eih"),
                    "robot_state": obs['robot0_joint_pos'],
                    "gripper_state": obs['robot0_gripper_width'][0] if 'robot0_gripper_width' in obs else 0.04
                }
                
                output_data = {
                    "joint_action": joint_action,
                    "gripper_action": gripper_action,
                    "gripper_width": obs['robot0_gripper_width'][0] if 'robot0_gripper_width' in obs else 0.04,
                    "full_action": cur_action
                }
                
                metadata = {
                    "inference_time": inference_time,
                    "step": step,
                    "timeout_count": timeout_count,
                    "n_action_steps": self.policy.policy.config.n_action_steps,
                    "chunk_size": self.policy.policy.config.chunk_size
                }
                
                # ä¿å­˜è®°å½•
                record_id = self.logger.log_inference(input_data, output_data, metadata)
                
                # å¼‚å¸¸æ£€æµ‹
                anomalies = self.detector.detect_anomalies(input_data, output_data, metadata)
                if anomalies:
                    print(f"âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸: {anomalies}")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                interface.execute_action(joint_action)
                interface.execute_gripper_action(gripper_action)
                
                # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                if step % 10 == 0:
                    current_time = time.monotonic() - t_start
                    avg_inference_time = np.mean(inference_times[-10:]) if len(inference_times) >= 10 else np.mean(inference_times)
                    print(f"Step {step}: æ—¶é—´={current_time:.2f}s, æ¨ç†æ—¶é—´={inference_time:.3f}s (å¹³å‡={avg_inference_time:.3f}s)")
                    print(f"  å…³èŠ‚åŠ¨ä½œ: {joint_action}")
                    print(f"  GripperåŠ¨ä½œ: {gripper_action}")
                    print(f"  è®°å½•ID: {record_id}")
                    if timeout_count > 0:
                        print(f"  è¶…æ—¶æ¬¡æ•°: {timeout_count}")
                    if anomalies:
                        print(f"  å¼‚å¸¸: {anomalies}")
                
                step += 1
                
                # ä½¿ç”¨precise_waitç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªå‘¨æœŸ
                precise_wait(t_cycle_end)
                
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢ç­–ç•¥...")
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ä¿å­˜ä¼šè¯æ€»ç»“
            print("\nä¿å­˜è°ƒè¯•æ•°æ®...")
            summary_file = self.logger.save_session_summary()
            stats = self.logger.get_stats()
            print(f"è°ƒè¯•æ•°æ®å·²ä¿å­˜:")
            print(f"  æ€»æ­¥æ•°: {stats['total_steps']}")
            print(f"  å·²ä¿å­˜æ­¥æ•°: {stats['saved_steps']}")
            print(f"  å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']:.3f}s")
            print(f"  å¼‚å¸¸æ£€æµ‹æ¬¡æ•°: {self.detector.get_anomaly_stats()['anomaly_count']}")
            print(f"  ä¼šè¯æ€»ç»“: {summary_file}")
            
            # åœæ­¢ç­–ç•¥æ¥å£
            if 'interface' in locals():
                print("åœæ­¢ç­–ç•¥æ¥å£...")
                interface.stop()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'camera_system'):
            self.camera_system.close()


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®é»˜è®¤å‚æ•°ï¼Œä¸éœ€è¦å‘½ä»¤è¡Œä¼ å‚
    args = type('Args', (), {
        'model_path': "./outputs/train/act_franka_dataset/checkpoints/050000",  # é»˜è®¤æ¨¡å‹è·¯å¾„
        'device': "cuda",  # é»˜è®¤ä½¿ç”¨GPU
        'config_path': "./config/robot_config.yaml",  # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
        'max_steps': 1000,  # é»˜è®¤æœ€å¤§æ­¥æ•°
        'test_mode': False,  # é»˜è®¤ä½¿ç”¨å®æ—¶æ¨¡å¼ï¼ˆå®‰å…¨ï¼‰
        'frequency': 10.0,  # é»˜è®¤æ¨ç†é¢‘ç‡
        'debug_image': False,  # é»˜è®¤ä¸æ˜¾ç¤ºå›¾åƒè°ƒè¯•ä¿¡æ¯
        'debug_log_dir': "debug_logs",  # é»˜è®¤è°ƒè¯•æ—¥å¿—ç›®å½•
        'save_frequency': 1  # é»˜è®¤æ¯æ­¥éƒ½ä¿å­˜
    })()
    
    print("ğŸ”§ ä½¿ç”¨é»˜è®¤å‚æ•°:")
    print(f"  æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"  è®¾å¤‡: {args.device}")
    print(f"  é…ç½®æ–‡ä»¶: {args.config_path}")
    print(f"  æœ€å¤§æ­¥æ•°: {args.max_steps}")
    print(f"  æµ‹è¯•æ¨¡å¼: {args.test_mode}")
    print(f"  æ¨ç†é¢‘ç‡: {args.frequency} Hz")
    print(f"  è°ƒè¯•æ—¥å¿—ç›®å½•: {args.debug_log_dir}")
    print(f"  ä¿å­˜é¢‘ç‡: æ¯{args.save_frequency}æ­¥")
    print("ğŸ’¡ å¦‚éœ€ä¿®æ”¹å‚æ•°ï¼Œè¯·ç›´æ¥ç¼–è¾‘è„šæœ¬ä¸­çš„é»˜è®¤å€¼")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config_path):
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config_path}")
        print("ğŸ’¡ è¯·ç¡®ä¿é…ç½®æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä¿®æ”¹è„šæœ¬ä¸­çš„é»˜è®¤è·¯å¾„")
        return 1
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        print("ğŸ’¡ è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä¿®æ”¹è„šæœ¬ä¸­çš„é»˜è®¤è·¯å¾„")
        return 1
    
    # åˆ›å»ºå¹¶è¿è¡ŒACTæ¨ç†è¿è¡Œå™¨
    try:
        runner = ACTInferenceRunner(
            model_path=args.model_path,
            config_path=args.config_path,
            device=args.device,
            max_steps=args.max_steps,
            test_mode=args.test_mode,
            frequency=args.frequency,
            debug_image=args.debug_image,
            debug_log_dir=args.debug_log_dir,
            save_frequency=args.save_frequency
        )
        
        # æ‰§è¡Œæ¨ç†
        runner.run()
        
    except Exception as e:
        print(f"æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        # æ¸…ç†èµ„æº
        if 'runner' in locals():
            runner.cleanup()
    
    print("æ¨ç†è„šæœ¬æ‰§è¡Œå®Œæˆ")
    return 0


if __name__ == "__main__":
    exit(main())
