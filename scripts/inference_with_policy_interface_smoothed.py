#!/usr/bin/env python3
"""
åŸºäºç›¸æœºå’ŒACTæ¨¡å‹çš„å®æ—¶æ¨ç†è„šæœ¬ - æ›´æ–°ç‰ˆæœ¬
é€‚é…æœ€æ–°ç‰ˆæœ¬çš„lerobotåº“
"""

import os
from shlex import join
import numpy as np
import torch
import time
from pathlib import Path
import argparse
from PIL import Image
import cv2
import math
from safetensors.torch import load_file
import sys
import yaml
from collections import deque

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.pathï¼Œç¡®ä¿ä¼˜å…ˆä½¿ç”¨é¡¹ç›®ä¸­çš„lerobotåº“
project_dir = Path(__file__).parent.parent
model_lerobot_path = project_dir / "model" / "lerobot" / "src"
sys.path.insert(0, str(model_lerobot_path))
sys.path.insert(0, str(project_dir))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

# åœ¨æ·»åŠ è·¯å¾„åå¯¼å…¥é¡¹ç›®æ¨¡å—
from common.gripper_util import convert_gripper_width_to_encoder

# å¯¼å…¥æœ€æ–°ç‰ˆæœ¬çš„lerobotåº“
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.constants import OBS_IMAGES, ACTION, OBS_STATE

# å¯¼å…¥PolicyInterface
from policy_interface import create_policy_interface

# å¯¼å…¥precise_wait
from common.precise_sleep import precise_wait

# ç›¸æœºç›¸å…³å¯¼å…¥
import pyrealsense2 as rs
from r3kit.devices.camera.realsense import config as rs_cfg
from r3kit.devices.camera.realsense.d415 import D415
R3KIT_RS_AVAILABLE = True

class ActionSmoother:
    """åŠ¨ä½œå¹³æ»‘å™¨ - æ£€æµ‹çªå˜å¹¶å¹³æ»‘åŠ¨ä½œ"""
    
    def __init__(self, mutation_threshold=0.1,history_size=10):
        """
        åˆå§‹åŒ–åŠ¨ä½œå¹³æ»‘å™¨
        
        Args:
            mutation_threshold: çªå˜é˜ˆå€¼ (rad)
            history_size: å†å²åŠ¨ä½œå­˜å‚¨å¤§å°
        """
        self.mutation_threshold = mutation_threshold
        self.history_size = history_size
        
        # å­˜å‚¨å†å²åŠ¨ä½œï¼ˆç”¨äºçªå˜æ£€æµ‹ï¼‰
        self.joint_history = deque(maxlen=history_size)
        self.step_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.mutation_count = 0
        self.total_steps = 0
        
    def smooth_action(self, joint_action):
        """
        å¹³æ»‘å…³èŠ‚åŠ¨ä½œ - ä¿®å¤ç‰ˆæœ¬
        
        Args:
            joint_action: 7ç»´å…³èŠ‚åŠ¨ä½œæ•°ç»„
            
        Returns:
            smoothed_action: å¹³æ»‘åçš„7ç»´å…³èŠ‚åŠ¨ä½œæ•°ç»„
        """
        self.total_steps += 1
        self.step_count += 1
        joint_action = np.array(joint_action)
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œç›´æ¥è¿”å›åŸå§‹åŠ¨ä½œå¹¶å­˜å‚¨
        if len(self.joint_history) < 1:
            self.joint_history.append(joint_action.copy())
            return joint_action
        
        # è®¡ç®—åŠ¨ä½œå˜åŒ–ç‡ï¼ˆä¸å†å²è®°å½•ä¸­çš„æœ€åä¸€ä¸ªåŠ¨ä½œæ¯”è¾ƒï¼‰
        prev_joint = self.joint_history[-1]  # ä½¿ç”¨å†å²è®°å½•ä¸­çš„æœ€åä¸€ä¸ªåŠ¨ä½œ
        curr_joint = joint_action
        change_vector = curr_joint - prev_joint
        change_rate = np.linalg.norm(change_vector)
        
        # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿçªå˜
        if change_rate > self.mutation_threshold:
            self.mutation_count += 1
            
            # è®¡ç®—ç¼©æ”¾å› å­ï¼Œä½¿å˜åŒ–ç‡ç­‰äºé˜ˆå€¼
            scale_factor = self.mutation_threshold / change_rate
            
            # ç¼©æ”¾å˜åŒ–å‘é‡ï¼Œä¿æŒæ–¹å‘ä¸å˜
            smoothed_change = change_vector * scale_factor
            smoothed_action = prev_joint + smoothed_change
            
            print(f"ğŸš¨ æ£€æµ‹åˆ°çªå˜! æ­¥éª¤: {self.step_count}")
            print(f"  åŸå§‹å˜åŒ–ç‡: {change_rate:.6f} rad")
            print(f"  ç¼©æ”¾å› å­: {scale_factor:.4f}")
            print(f"  å¹³æ»‘åå˜åŒ–ç‡: {np.linalg.norm(smoothed_change):.6f} rad")
            print(f"  ä¸»è¦å˜åŒ–å…³èŠ‚: {self._find_max_change_joint(change_vector)}")
            print("-" * 40)
            
            # å­˜å‚¨å¹³æ»‘åçš„åŠ¨ä½œåˆ°å†å²è®°å½•
            self.joint_history.append(smoothed_action.copy())
            return smoothed_action
        else:
            # æ²¡æœ‰çªå˜ï¼Œå­˜å‚¨åŸå§‹åŠ¨ä½œå¹¶è¿”å›
            self.joint_history.append(joint_action.copy())
            return joint_action
    
    def _find_max_change_joint(self, change_vector):
        """æ‰¾åˆ°å˜åŒ–æœ€å¤§çš„å…³èŠ‚"""
        change_vector = np.array(change_vector)
        max_joint_idx = np.argmax(np.abs(change_vector))
        return f"å…³èŠ‚{max_joint_idx+1} (å˜åŒ–: {change_vector[max_joint_idx]:.4f} rad)"
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if self.total_steps == 0:
            return {
                'total_steps': 0,
                'mutation_count': 0,
                'mutation_rate': 0.0
            }
        
        return {
            'total_steps': self.total_steps,
            'mutation_count': self.mutation_count,
            'mutation_rate': self.mutation_count / self.total_steps
        }
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics()
        print(f"\n=== åŠ¨ä½œå¹³æ»‘ç»Ÿè®¡ ===")
        print(f"æ€»æ­¥æ•°: {stats['total_steps']}")
        print(f"çªå˜æ¬¡æ•°: {stats['mutation_count']}")
        print(f"çªå˜æ¯”ä¾‹: {stats['mutation_rate']:.2%}")
        print(f"çªå˜é˜ˆå€¼: {self.mutation_threshold} rad")

# D415 ç›¸æœºé…ç½®ï¼ˆä¸é‡‡é›†è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
FPS = 30
D415_CAMERAS = {   
    "cam4": "327322062498",  # å›ºå®šæœºä½è§†è§’
    "eih": "038522062288",   # eye-in-handè§†è§’ï¼ˆéœ€è¦æ ¹æ®å®é™…åºåˆ—å·ä¿®æ”¹ï¼‰
}

class CameraSystem:
    """ç›¸æœºç³»ç»Ÿæ¥å£ - ä»inference_poly1å¤ç”¨"""
    
    def __init__(self):
        self.cameras = {}
        self.camera_names = ["cam4", "eih"]  # æ”¯æŒåŒè§†è§’
        self.use_realsense = True
        
        # ä¸é‡‡é›†è„šæœ¬ä¿æŒä¸€è‡´çš„æµé…ç½®
        rs_cfg.D415_STREAMS = [
            (rs.stream.depth, 640,480, rs.format.z16, FPS),
            (rs.stream.color, 640,480, rs.format.bgr8, FPS),
        ]
        for name in self.camera_names:
            serial = D415_CAMERAS.get(name)
            if serial is None:
                print(f"{name} ç¼ºå°‘åºåˆ—å·ï¼Œè·³è¿‡")
                continue
            try:
                cam = D415(id=serial, depth=True, name=name)
                self.cameras[name] = cam
                print(f"æˆåŠŸåˆå§‹åŒ–ç›¸æœº {name} (åºåˆ—å·: {serial})")
            except Exception as e:
                print(f"åˆå§‹åŒ–ç›¸æœº {name} å¤±è´¥: {e}")
                continue
                
        if len(self.cameras) > 0:
            self.use_realsense = True
            print(f"ä½¿ç”¨ RealSense D415ï¼Œç›¸æœºæ•°é‡: {len(self.cameras)}")
            print(f"å¯ç”¨ç›¸æœº: {list(self.cameras.keys())}")
        else:
            print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸåˆå§‹åŒ–ä»»ä½•ç›¸æœº")
    
    def get_image(self, cam_name):
        """è·å–æŒ‡å®šç›¸æœºçš„å›¾åƒ"""
        if cam_name not in self.cameras:
            return None
        
        try:
            if self.use_realsense:
                # r3kit D415 æ¥å£
                color, depth = self.cameras[cam_name].get()
                if color is None:
                    return None
                # è½¬ RGBï¼ˆä¸‹æ¸¸é¢„å¤„ç†é»˜è®¤ä»¥ RGB å¤„ç†ï¼‰
                frame_rgb = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)
                return frame_rgb
            else:
                # OpenCV æ‘„åƒå¤´
                ret, frame = self.cameras[cam_name].read()
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    return frame_rgb
                return None
        except Exception as e:
            print(f"è·å– {cam_name} å›¾åƒå¤±è´¥: {e}")
            return None
    
    def get_all_images(self):
        """è·å–æ‰€æœ‰ç›¸æœºçš„å›¾åƒ"""
        images = {}
        for cam_name in self.camera_names:
            image = self.get_image(cam_name)
            if image is not None:
                images[cam_name] = image
            else:
                # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒä½œä¸º fallback
                images[cam_name] = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                print(f"è­¦å‘Š: {cam_name} ç›¸æœºå›¾åƒè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾åƒ")
        
        return images
    
    def close(self):
        """å…³é—­æ‰€æœ‰ç›¸æœº"""
        for cam_name, cap in self.cameras.items():
            try:
                if self.use_realsense:
                    # D415 ç±»å¯èƒ½æ²¡æœ‰ stop æ–¹æ³•ï¼Œä½¿ç”¨ __del__ æˆ–è€…ä¸åšä»»ä½•æ“ä½œ
                    if hasattr(cap, 'stop'):
                        cap.stop()
                    elif hasattr(cap, 'close'):
                        cap.close()
                    # å¯¹äº r3kit D415ï¼Œé€šå¸¸ç”±ææ„å‡½æ•°è‡ªåŠ¨å¤„ç†
                else:
                    cap.release()
                print(f"{cam_name} å·²å…³é—­")
            except Exception as e:
                print(f"å…³é—­ {cam_name} å¤±è´¥: {e}")


class ACTPolicyWrapper:
    """ACTç­–ç•¥åŒ…è£…å™¨ - é€‚é…æœ€æ–°ç‰ˆæœ¬çš„lerobotåº“"""
    
    def __init__(self, model_path, device="cpu", camera_system=None, debug_image=False,use_eih=True):
        self.device = torch.device(device)
        self.model_path = Path(model_path)
        self.camera_system = camera_system
        self.debug_image = debug_image
        self.use_eih = use_eih
        # é…ç½®å‚æ•°
        self.image_size = (224, 224)
        self.camera_names = ["cam4"]  # é»˜è®¤åªæœ‰å›ºå®šæœºä½è§†è§’
        if self.use_eih:
            self.camera_names.append("eih")  # å¦‚æœéœ€è¦eihï¼Œæ·»åŠ åˆ°ç›¸æœºåˆ—è¡¨
        self.joint_dim = 7  # 7ä¸ªå…³èŠ‚è§’åº¦ï¼ˆå¼§åº¦ï¼‰  
        self.gripper_dim = 1  # 1ä¸ªå¤¹çˆªå¼€åˆå€¼  
        self.action_dim = self.joint_dim + self.gripper_dim  # æ€»å…±8ç»´  
        self.chunk_size = 100  # ACTæ¨¡å‹çš„chunkå¤§å°
        
        # åŠ è½½æ¨¡å‹
        self.policy = self._load_policy()
        
        print(f"ACTç­–ç•¥åˆå§‹åŒ–å®Œæˆ: {model_path}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ”¯æŒè§†è§’: å›ºå®šæœºä½(cam4)" + (" + eye-in-hand(eih)" if self.use_eih else ""))
        print(f"ç›¸æœºç³»ç»ŸçŠ¶æ€: {len(self.camera_system.cameras) if self.camera_system else 0} ä¸ªç›¸æœºå·²åˆå§‹åŒ–")
    
    def _load_policy(self):
        """åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹"""
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        # ä½¿ç”¨from_pretrainedåŠ è½½æ¨¡å‹(æ¨èæ–¹å¼)
        policy = ACTPolicy.from_pretrained(
            pretrained_name_or_path=str(self.model_path)
        )
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        policy.to(self.device)
        
        # è®¾ç½®æ‰§è¡Œéƒ¨ç½²
        policy.config.n_action_steps = 50

        # æ‰“å°é…ç½®ä¿¡æ¯
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸ:")
        print(f" ç­–ç•¥ç±»å‹: {policy.config.type}")
        print(f" è®¾å¤‡: {next(policy.parameters()).device}")
        print(f" æ—¶é—´é›†æˆç³»æ•°: {policy.config.temporal_ensemble_coeff}")
        print(f" åŠ¨ä½œæ­¥æ•°: {policy.config.n_action_steps}")
        print(f" å—å¤§å°: {policy.config.chunk_size}")
        
        return policy
    
    def preprocess_image(self, image, debug=False):
        """é¢„å¤„ç†å›¾åƒ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼šå…ˆè£å‰ªæˆæ­£æ–¹å½¢ï¼Œå†ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸"""
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        # è·å–åŸå§‹å›¾åƒå°ºå¯¸
        width, height = image.size
        if debug:
            print(f"åŸå§‹å›¾åƒå°ºå¯¸: {width}x{height}")
        
        # æŒ‰ç…§è®­ç»ƒæ—¶çš„å¤„ç†æ–¹å¼è£å‰ª
        if width == 640 and height == 480:
            # 640*480å°ºå¯¸ï¼šä»ç‰¹å®šä½ç½®è£å‰ªåˆ°360*360
            left = 200
            right = 560
            top = 0
            bottom = 360
            if debug:
                print(f"640x480å›¾ç‰‡ï¼Œè£å‰ªåŒºåŸŸ: ({left}, {top}, {right}, {bottom})")
        else:
            # å…¶ä»–å°ºå¯¸ï¼šæŒ‰æ¯”ä¾‹è£å‰ªæˆæ­£æ–¹å½¢
            min_dim = min(width, height)
            left = (width - min_dim) // 2
            right = left + min_dim
            top = (height - min_dim) // 2
            bottom = top + min_dim
            if debug:
                print(f"å…¶ä»–å°ºå¯¸å›¾ç‰‡ï¼Œè£å‰ªæˆæ­£æ–¹å½¢: ({left}, {top}, {right}, {bottom})")
        
        # è£å‰ª
        image_cropped = image.crop((left, top, right, bottom))
        if debug:
            print(f"è£å‰ªåå°ºå¯¸: {image_cropped.size}")
        
        # ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
        image_resized = image_cropped.resize(self.image_size, Image.Resampling.LANCZOS)
        if debug:
            print(f"ç¼©æ”¾åå°ºå¯¸: {image_resized.size}")
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
        image_tensor = torch.from_numpy(np.array(image_resized)).permute(2, 0, 1).float()  # (3, H, W)
        image_tensor = image_tensor / 255.0
        
        return image_tensor
    
    def get_current_state_with_gripper(self, obs):
        """ä»è§‚æµ‹ä¸­è·å–å½“å‰çŠ¶æ€ï¼ˆ8ç»´ï¼‰"""
        # ä»è§‚æµ‹ä¸­æå–å…³èŠ‚ä½ç½®ï¼ˆå¼§åº¦ï¼‰
        joints_rad = obs['robot0_joint_pos']  # (7,)
        
        # è·å–å¤¹çˆªå®½åº¦ï¼ˆä»è§‚æµ‹ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        if 'robot0_gripper_width' in obs:
            gripper_width = obs['robot0_gripper_width']
            if isinstance(gripper_width, np.ndarray):
                gripper_width = gripper_width[0] if len(gripper_width) > 0 else 0.04
        else:
            gripper_width = 0.04  # é»˜è®¤å¤¹çˆªå®½åº¦ï¼ˆç±³ï¼‰
        
        # è¿”å›8ç»´çŠ¶æ€ï¼š7ä¸ªå…³èŠ‚è§’åº¦ï¼ˆå¼§åº¦ï¼‰ + 1ä¸ªå¤¹çˆªå®½åº¦ï¼ˆç±³ï¼‰
        return np.concatenate([joints_rad, [gripper_width]])
    
    def predict_single_action(self, images, current_state):
        """
        å•æ­¥é¢„æµ‹åŠ¨ä½œï¼ˆä½¿ç”¨ ACTPolicy.select_actionï¼‰ã€‚
        è¿”å›: (8,) numpy æ•°ç»„ï¼Œå‰7ç»´ä¸ºå…³èŠ‚(å¼§åº¦)ï¼Œç¬¬8ç»´ä¸ºå¤¹çˆª(ç±³)ã€‚
        """
        # é¢„å¤„ç†å›ºå®šæœºä½è§†è§’å›¾åƒ
        if "cam4" in images:
            color_img_tensor = self.preprocess_image(images["cam4"], debug=self.debug_image)
        else:
            # éšæœºå›¾åƒå›é€€
            fake = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            color_img_tensor = self.preprocess_image(fake, debug=self.debug_image)
            print("è­¦å‘Š: å›ºå®šæœºä½è§†è§’å›¾åƒè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾åƒ")
        
        # æ„å»ºbatch - æ ¹æ®æ˜¯å¦ä½¿ç”¨eihæ¥å†³å®šè¾“å…¥æ ¼å¼
        if self.use_eih:
            # é¢„å¤„ç†eye-in-handè§†è§’å›¾åƒ
            if "eih" in images:
                eih_img_tensor = self.preprocess_image(images["eih"], debug=self.debug_image)
            else:
                # éšæœºå›¾åƒå›é€€
                fake = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                eih_img_tensor = self.preprocess_image(fake, debug=self.debug_image)
                print("è­¦å‘Š: eye-in-handè§†è§’å›¾åƒè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾åƒ")
            
            batch = {
                "observation.images.cam": color_img_tensor.unsqueeze(0).to(self.device),
                "observation.images.eih": eih_img_tensor.unsqueeze(0).to(self.device),
                "observation.state": torch.tensor(current_state, dtype=torch.float32).unsqueeze(0).to(self.device),
            }
        else:
            # åªä½¿ç”¨å›ºå®šæœºä½è§†è§’
            batch = {
                "observation.images.cam": color_img_tensor.unsqueeze(0).to(self.device),
                "observation.state": torch.tensor(current_state, dtype=torch.float32).unsqueeze(0).to(self.device),
            }
        
        with torch.no_grad():
            # ä½¿ç”¨select_actionè¿›è¡Œå•æ­¥é¢„æµ‹
            action = self.policy.select_action(batch)  # (1, action_dim)ï¼Œå·²åå½’ä¸€åŒ–
            action = action.squeeze(0).detach().cpu().numpy()  # (8,)
        
        # ç›´æ¥è¿”å›æ¨¡å‹è¾“å‡ºï¼Œæ‰€æœ‰å•ä½éƒ½æ˜¯å¼§åº¦ï¼ˆå‰7ç»´ï¼‰å’Œç±³ï¼ˆç¬¬8ç»´ï¼‰
        return action
    
    
    def __call__(self, obs):
        """
        ç­–ç•¥å‡½æ•° - PolicyInterfaceå…¼å®¹æ¥å£
        
        Args:
            obs: è§‚æµ‹å­—å…¸ï¼ŒåŒ…å«robot0_joint_posç­‰
            
        Returns:
            action: 7ç»´å…³èŠ‚åŠ¨ä½œ [j1, j2, j3, j4, j5, j6, j7] (å¼§åº¦)
        """
        # è·å–å½“å‰å›¾åƒ
        current_images = self.camera_system.get_all_images()
        
        # è·å–å½“å‰çŠ¶æ€
        current_state = self.get_current_state_with_gripper(obs)
        
        # å•æ­¥é¢„æµ‹åŠ¨ä½œ
        full_action = self.predict_single_action(current_images, current_state)
        #print(f"é¢„æµ‹çš„å®Œæ•´åŠ¨ä½œï¼ˆ8ç»´ï¼‰: {full_action}")
        
        joint_action = full_action[:self.joint_dim]
        # è·å–gripperåŠ¨ä½œï¼ˆç¬¬8ç»´ï¼‰
        gripper_width = full_action[self.joint_dim] #+ 0.05 # å¤¹çˆªå®½åº¦ï¼ˆç±³ï¼‰

        # if gripper_width > 0.035:
        #     gripper_width *= 1.5
        # elif gripper_width < 0.025:
        #     gripper_width *= 0.7
        #gripper_width = 0.08

        cur_action = np.concatenate([joint_action, [gripper_width]])
        return cur_action
    
    def check_camera_status(self):
        """æ£€æŸ¥ç›¸æœºçŠ¶æ€"""
        if not self.camera_system:
            print("ç›¸æœºç³»ç»Ÿæœªåˆå§‹åŒ–")
            return False
        
        print("ç›¸æœºçŠ¶æ€æ£€æŸ¥:")
        for cam_name in self.camera_names:
            if cam_name in self.camera_system.cameras:
                print(f"  âœ… {cam_name}: å·²åˆå§‹åŒ–")
            else:
                print(f"  âŒ {cam_name}: æœªåˆå§‹åŒ–")
        
        return len(self.camera_system.cameras) > 0


class ACTInferenceRunner:
    """ACTæ¨ç†è¿è¡Œå™¨ - ä½¿ç”¨ä¸replay_trajectoryç›¸åŒçš„æ¥å£å½¢å¼"""
    
    def __init__(self, 
                 model_path: str,
                 config_path: str,
                 device: str = "cuda",
                 max_steps: int = 1000,
                 test_mode: bool = False,
                 frequency: float = 20.0,
                 debug_image: bool = False,
                 use_eih: bool = True):
        """
        åˆå§‹åŒ–ACTæ¨ç†è¿è¡Œå™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            max_steps: æœ€å¤§è¿è¡Œæ­¥æ•°
            test_mode: æµ‹è¯•æ¨¡å¼
            frequency: æ¨ç†é¢‘ç‡ (Hz)
        """
        self.model_path = model_path
        self.config_path = config_path
        self.device = device
        self.max_steps = max_steps
        self.test_mode = test_mode
        self.frequency = frequency
        self.debug_image = debug_image
        self.use_eih = use_eih
        self.dt = 1.0 / frequency  # æ—¶é—´é—´éš”
        
        # åˆ›å»ºç›¸æœºç³»ç»Ÿ
        self.camera_system = CameraSystem()
        
        # åˆ›å»ºACTç­–ç•¥
        self.policy = ACTPolicyWrapper(
            model_path=model_path,
            device=device,
            camera_system=self.camera_system,
            debug_image=self.debug_image,
            use_eih=self.use_eih
        )
        
        # åˆ›å»ºåŠ¨ä½œå¹³æ»‘å™¨
        self.action_smoother = ActionSmoother(mutation_threshold=0.01,history_size=10)
        
        print(f"ACTæ¨ç†è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"é…ç½®æ–‡ä»¶: {config_path}")
        print(f"è®¾å¤‡: {device}")
        print(f"æµ‹è¯•æ¨¡å¼: {test_mode}")
        print(f"æ¨ç†é¢‘ç‡: {frequency} Hz")
        print(f"ä½¿ç”¨eih: {self.use_eih}")
        
        # æ£€æŸ¥ç›¸æœºçŠ¶æ€
        self.policy.check_camera_status()
    
    def run(self):
        """æ‰§è¡Œæ¨ç†"""
        if self.test_mode:
            print("ä½¿ç”¨æµ‹è¯•æ¨¡å¼")
            self._run_test_mode()
        else:
            print("ä½¿ç”¨å®æ—¶æ¨ç†æ¨¡å¼")
            self._run_real_time_mode()
    
    def _run_test_mode(self):
        """æµ‹è¯•æ¨¡å¼ï¼šè¿è¡Œå‡ æ¬¡æ¨ç†"""
        print("å¼€å§‹æµ‹è¯•æ¨ç†...")
        for i in range(3):
            print(f"\n=== æµ‹è¯•æ¨ç† {i + 1} ===")
            # æ¨¡æ‹Ÿè§‚æµ‹æ•°æ®
            obs = {
                'robot0_joint_pos': np.random.uniform(-1, 1, 7),
                'robot0_joint_vel': np.random.uniform(-0.1, 0.1, 7),
                'robot0_eef_pos': np.random.uniform(0.3, 0.7, 3),
                'robot0_eef_rot_axis_angle': np.random.uniform(-1, 1, 3),
                'robot0_gripper_width': np.random.uniform(0.0, 0.08, 1),  # æ·»åŠ gripperå®½åº¦
                'timestamp': time.monotonic()
            }
            
            # æ‰§è¡Œç­–ç•¥
            cur_action = self.policy(obs)
            joint_action = cur_action[:self.policy.joint_dim]
            gripper_action = cur_action[self.policy.joint_dim]
            
            print(f"é¢„æµ‹çš„å…³èŠ‚åŠ¨ä½œï¼ˆ7ç»´ï¼‰: {joint_action}")
            print(f"é¢„æµ‹çš„å¤¹çˆªåŠ¨ä½œï¼ˆ1ç»´ï¼‰: {gripper_action}")
            print(f"é¢„æµ‹çš„å®Œæ•´åŠ¨ä½œï¼ˆ8ç»´ï¼‰: {cur_action}")
            print(f"é¢„æµ‹çš„å¤¹çˆªåŠ¨ä½œ: {gripper_action}")
            
            time.sleep(2)
    
    def _run_real_time_mode(self):
        """å®æ—¶æ¨ç†æ¨¡å¼"""
        try:
            # åˆ›å»ºç­–ç•¥æ¥å£
            interface = create_policy_interface(self.config_path, self.policy)
            
            print("å¯åŠ¨ç­–ç•¥æ¥å£...")
            interface.start()
            print("ç­–ç•¥æ¥å£å·²å¯åŠ¨!")
            
            # è·å–åˆå§‹è§‚æµ‹
            obs = interface.get_observation()
            print(f"åˆå§‹å…³èŠ‚ä½ç½®: {obs['robot0_joint_pos']}")
            print(f"åˆå§‹Gripperå®½åº¦: {obs['robot0_gripper_width']}")
            
            # è¿è¡Œç­–ç•¥
            print(f"\nå¼€å§‹è¿è¡Œç­–ç•¥...")
            print(f"æ¨ç†é¢‘ç‡: {self.frequency} Hz (dt = {self.dt:.3f}s)")
            print("æŒ‰ Ctrl+C åœæ­¢")
            
            # åˆå§‹åŒ–æ—¶é—´æ§åˆ¶
            t_start = time.monotonic()
            step = 0
            
            # è¶…æ—¶é™çº§ç­–ç•¥ç›¸å…³å˜é‡
            last_joint_action = None
            last_gripper_action = None
            inference_times = []
            max_inference_time = 0.18  # æœ€å¤§å…è®¸æ¨ç†æ—¶é—´ (180ms) - é’ˆå¯¹130msæ¨ç†æ—¶é—´ä¼˜åŒ–
            timeout_count = 0
            
            while True:
                if self.max_steps is not None and step >= self.max_steps:
                    print(f"è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.max_steps}ï¼Œåœæ­¢è¿è¡Œ")
                    break
                
                # è®¡ç®—å½“å‰å‘¨æœŸç»“æŸæ—¶é—´
                t_cycle_end = t_start + (step + 1) * self.dt
                t_cycle_start = time.monotonic()
                
                # è·å–è§‚æµ‹
                obs = interface.get_observation()
                
                # æ‰§è¡Œç­–ç•¥ - æ·»åŠ è¶…æ—¶æ£€æŸ¥
                t_inference_start = time.monotonic()
                try:
                    cur_action = self.policy(obs)
                    joint_action = cur_action[:self.policy.joint_dim]
                    gripper_action = cur_action[self.policy.joint_dim]
                    t_inference_end = time.monotonic()
                    inference_time = t_inference_end - t_inference_start
                    inference_times.append(inference_time)
                    
                    # æ›´æ–°æœ€åæœ‰æ•ˆçš„åŠ¨ä½œ
                    last_joint_action = joint_action.copy()  # ä¿å­˜7ç»´å…³èŠ‚åŠ¨ä½œ
                    last_gripper_action = gripper_action.copy()
                    timeout_count = 0
                    
                except Exception as e:
                    print(f"æ¨ç†å¤±è´¥: {e}")
                    t_inference_end = time.monotonic()
                    inference_time = t_inference_end - t_inference_start
                    inference_times.append(inference_time)
                    timeout_count += 1
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.monotonic()
                elapsed_time = current_time - t_cycle_start
                remaining_time = t_cycle_end - current_time
                
                # å¦‚æœæ¨ç†æ—¶é—´è¿‡é•¿æˆ–å‰©ä½™æ—¶é—´ä¸è¶³ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
                if (inference_time > max_inference_time or 
                    remaining_time < 0.01): # å‰©ä½™æ—¶é—´å°‘äº10ms):
                    
                    if last_joint_action is not None and last_gripper_action is not None:
                        # ä½¿ç”¨ä¸Šæ¬¡çš„æœ‰æ•ˆåŠ¨ä½œ
                        joint_action = last_joint_action
                        gripper_action = last_gripper_action
                        print(f"âš ï¸  ä½¿ç”¨é™çº§ç­–ç•¥: æ¨ç†æ—¶é—´={inference_time:.3f}s, å‰©ä½™æ—¶é—´={remaining_time:.3f}s")
                    else:
                        # ä¸è¦ä½¿ç”¨å½“å‰ä½ç½®ï¼Œè€Œæ˜¯è·³è¿‡è¿™æ¬¡æ‰§è¡Œ
                        joint_action = obs['robot0_joint_pos'] + np.random.normal(0, 0.001, 7)
                        print(f"âš ï¸  ä½¿ç”¨éšæœºæ‰°åŠ¨åŠ¨ä½œï¼Œç­‰å¾…æœ‰æ•ˆæ¨ç†: æ¨ç†æ—¶é—´={inference_time:.3f}s")
                        continue  # è·³è¿‡è¿™æ¬¡å¾ªç¯
                
                # åŠ¨ä½œå¹³æ»‘å¤„ç†
                smoothed_joint_action = self.action_smoother.smooth_action(joint_action)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                interface.execute_action(smoothed_joint_action)
                interface.execute_gripper_action(gripper_action)
                
                # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                if step % 10 == 0:
                    current_time = time.monotonic() - t_start
                    avg_inference_time = np.mean(inference_times[-10:]) if len(inference_times) >= 10 else np.mean(inference_times)
                    print(f"Step {step}: æ—¶é—´={current_time:.2f}s, æ¨ç†æ—¶é—´={inference_time:.3f}s (å¹³å‡={avg_inference_time:.3f}s)")
                    print(f"  å…³èŠ‚åŠ¨ä½œ: {joint_action}")
                    print(f"  GripperåŠ¨ä½œ: {gripper_action}")
                    if timeout_count > 0:
                        print(f"  è¶…æ—¶æ¬¡æ•°: {timeout_count}")
                
                step += 1
                
                # ä½¿ç”¨precise_waitç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªå‘¨æœŸ
                precise_wait(t_cycle_end)
                
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢ç­–ç•¥...")
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # æ‰“å°åŠ¨ä½œå¹³æ»‘ç»Ÿè®¡ä¿¡æ¯
            self.action_smoother.print_statistics()
            
            # åœæ­¢ç­–ç•¥æ¥å£
            if 'interface' in locals():
                print("åœæ­¢ç­–ç•¥æ¥å£...")
                interface.stop()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'camera_system'):
            self.camera_system.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŸºäºç›¸æœºå’ŒACTæ¨¡å‹çš„å®æ—¶æ¨ç†è„šæœ¬ - æ›´æ–°ç‰ˆæœ¬")
    parser.add_argument("--model_path", type=str, 
                       default="/home/robotflow/Downloads/060000/pretrained_model",
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¡ç®—è®¾å¤‡ (cpu/cuda)")
    parser.add_argument("--config_path", type=str,
                       default="/home/robotflow/my_code/other_codes/franka_control_final/config/robot_config.yaml",
                       help="æœºå™¨äººé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max_steps", type=int, default=1000,
                       help="æœ€å¤§è¿è¡Œæ­¥æ•°")
    parser.add_argument("--test_mode", action="store_true", default=False,
                       help="æµ‹è¯•æ¨¡å¼ï¼ˆä¸è¿æ¥çœŸå®æœºå™¨äººï¼‰")
    parser.add_argument("--frequency", type=float, default=10.0,
                       help="æ¨ç†é¢‘ç‡ (Hz) - é’ˆå¯¹130msæ¨ç†æ—¶é—´ä¼˜åŒ–")
    parser.add_argument("--debug_image", action="store_true", default=False,
                       help="æ˜¾ç¤ºå›¾åƒå¤„ç†è°ƒè¯•ä¿¡æ¯")
    parser.add_argument("--use_eih", action="store_true", default=False,  # æ–°å¢
                       help="ä½¿ç”¨eye-in-handè§†è§’ä½œä¸ºè¾“å…¥")
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config_path):
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config_path}")
        return 1
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return 1
    
    # åˆ›å»ºå¹¶è¿è¡ŒACTæ¨ç†è¿è¡Œå™¨
    try:
        runner = ACTInferenceRunner(
            model_path=args.model_path,
            config_path=args.config_path,
            device=args.device,
            max_steps=args.max_steps,
            test_mode=args.test_mode,
            frequency=args.frequency,
            debug_image=args.debug_image,
            use_eih=args.use_eih
        )
        
        # æ‰§è¡Œæ¨ç†
        runner.run()
        
    except Exception as e:
        print(f"æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        # æ¸…ç†èµ„æº
        if 'runner' in locals():
            runner.cleanup()
    
    print("æ¨ç†è„šæœ¬æ‰§è¡Œå®Œæˆ")
    return 0


if __name__ == "__main__":
    exit(main())
